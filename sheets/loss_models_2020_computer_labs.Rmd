---
title: "Loss models 2020: computer labs in R"
author: "Katrien Antonio & Jonas Crevecoeur"
date: '[Loss models 2020 labs](https://www.github.com/katrienantonio/loss-models-2020) | December 14 & 16, 2020'
output:
  xaringan::moon_reader:
    css:
    - default
    - css/metropolis.css
    - css/metropolis-fonts.css
    - css/my-css.css
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      highlightLines: yes
      countIncrementalSlides: no
      highlightSpans: yes
  html_document:
    df_print: paged
subtitle: A hands-on workshop <html><div style='float:left'></div><hr
  align='center' color='#116E8A' size=1px width=97%></html>
graphics: yes
editor_options:
  chunk_output_type: console
header-includes:
- \usepackage{tikz}
- \usetikzlibrary{shapes.geometric,shapes, snakes, arrows}
- \usepackage{amsfonts}
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{color}
- \usepackage{graphicx}
- \newcommand{\bTheta}{\ensuremath{\bs{\Theta}}}
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# options(knitr.table.format = "html")
library(tidyverse)
library(fontawesome) # from github: https://github.com/rstudio/fontawesome
library(DiagrammeR)
library(emo) # from github: https://github.com/hadley/emo
library(gt) # from github: https://github.com/rstudio/gt
library(countdown) # from github: https://github.com/gadenbuie/countdown 
library(here)
```

```{r setup_greenwell, include=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE, 
        crayon.enabled = TRUE)

# Set global knitr chunk options
knitr::opts_chunk$set(
  dev = "svg",
  fig.align = "center",
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)

# colors - I copied most of these from # https://github.com/edrubin/EC524W20
dark2 <- RColorBrewer::brewer.pal(8, name = "Dark2")
KULbg <- "#116E8A"
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#3b3b9a"
green      = "#8bb174"
grey_light = "grey70"
grey_mid   = "grey50"
grey_dark  = "grey20"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

class: inverse, center, middle
name: prologue

# Prologue

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

name: introduction

# Introduction

### Course

`r fa(name = "github", fill = KULbg)` https://github.com/katrienantonio/loss-models-2020

The course repo on GitHub, where you can find the data sets, lecture sheets, R scripts and R markdown files.

--

### Us

`r fa(name = "link", fill = KULbg)` [https://katrienantonio.github.io/](https://katrienantonio.github.io/)

`r fa(name = "paper-plane", fill = KULbg)` [katrien.antonio@kuleuven.be](mailto:katrien.antonio@kuleuven.be) & [jonas.crevecoeur@kuleuven.be](mailto:jonas.crevecoeur@kuleuven.be) 

`r fa('graduation-cap', fill = KULbg)` (Katrien, PhD) Professor in insurance data science at KU Leuven and University of Amsterdam

`r fa('graduation-cap', fill = KULbg)` (Jonas, PhD) Post-doctoral researcher in biostatistics at KU Leuven

---

name: checklist

# Checklist

☑ Do you have a fairly recent version of R?
  ```{r eval=TRUE}
  version$version.string
  ```

☑ Do you have a fairly recent version of RStudio? 
  ```{r eval=FALSE}
  RStudio.Version()$version
  ## Requires an interactive session but should return something like "[1] ‘1.3.1093’"
  ```

☑ Have you installed the R packages listed in the software requirements? 

or

☑ Have you created an account on RStudio Cloud (to avoid any local installation issues)?
  
---

name: why-this-course # inspired by Grant McDermott intro lecture

# Why this training?

### The goals of this training .font140[`r fa(name = "fas fa-rocket", fill = KULbg)`]

--

* develop practical .KULbginline[data handling foundations]

--

* visualize and explore data 

--

* cover essential actuarial modelling tasks, including fitting loss models for frequency and severity data

--

* learn by doing, get you started (in particular when you have limited experience in R). 

--

<br>

> *"In short, we will cover things that we wish someone had taught us in our undergraduate programs."* 
> <br>
> .font80[This quote is from the [Data science for economists course](http://github.com/uo-ec607/lectures) by Grant McDermott.]

---

name: universe

# Why R and RStudio? 

### Data science positivism

- Next to Python, R has become the *de facto* language for data science, with a cutting edge *machine learning toolbox*.
- See: [The Popularity of Data Science Software](http://r4stats.com/articles/popularity/)
- R is open-source with a very active community of users spanning academia and industry.

--

### Bridge to actuarial science, econometrics and other tools

- R has all of the statistics and econometrics support, and is amazingly adaptable as a “glue” language to other programming languages and APIs.
- R does not try to be everything to everyone. The RStudio IDE and ecosystem allow for further, seemless integration (with e.g. python, keras, tensorflow or C).
- Widely used in actuarial undergraduate programs 

--

### Disclaimer + Read more

- It's also the language that we know best.
- If you want to read more: [R-vs-Python](https://blog.rstudio.com/2019/12/17/r-vs-python-what-s-the-best-for-language-for-data-science/), [when to use Python or R](https://www.datacamp.com/community/blog/when-to-use-python-or-r) or [Hadley Wickham on the future of R](https://qz.com/1661487/hadley-wickham-on-the-future-of-r-python-and-the-tidyverse/)

---

class: clear, center, middle

background-image: url("image/tidyverse2.1.png")
background-size: cover
background-size: 65% 
background-position: center

---

name: tidyverse

# Welcome to the tidyverse!

><p align="justify">The .KULbginline[tidyverse] is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. </p>

<center>
<img src="image/tidyverse_wide.png" width="750"/>
</center>

More on: [tidyverse](https://www.tidyverse.org).

Install the packages with `install.packages("tidyverse")`. Then run `library(tidyverse)` to load the core tidyverse.

```{r echo=FALSE}
library(tidyverse)
```


---


# Today's Outline

.pull-left[

* [Prologue](#prologue)

* [What's out there: the R universe](#universe) (see the prework)

  - why R and RStudio?
  - welcome to the tidyverse!
  - principles of tidy data
  - workflow of a data scientist

* [Data wrangling and visualisation](#tidyverse) (see the prework)

  - tibbles
  - pipe operator `%>%`
  - {dplyr} instructions
  - {ggplot2} for data visualisation
  - what else is there?

]

.pull-right[

* [Data sets used in the session](#data-sets)

  - MTPL data on frequency and severity of claims
  - Secura Re losses
  
* [Fitting frequency models in R](#frequency)

  - frequency - severity approach
  - the $(a,b,0)$ class
  - MLE, model selection and evaluation tools
  - excess zeroes
 
* [Fitting severity models in R](#severity)

  - from simple to complex parametric distributions
  - be aware of truncation and censoring
  - a global fit via splicing
  - case-study on the Secura Re losses
  - what else is there?
  
]


---


class: inverse, center, middle
name: data-sets

# Data sets used in the session 

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>


---

# Data sets used in this session - MTPL <img src="image/pipe.png" class="title-hex"> <img src="image/dplyr.png" class="title-hex"> <img src="image/ggplot2.png" class="title-hex">

We illustrate some first data handling steps on the Motor Third Party Liability data set. There are 163 231 policyholders in this data set. 

The frequency of claiming (`nclaims`) and corresponding severity (`avg`, the amount paid on average per claim reported by a policyholder) are the .KULbginline[target variables] in this data set. 

Predictor variables are: 

* the exposure-to-risk, the duration of the insurance coverage (max. 1 year)
* factor variables, e.g. gender, coverage, fuel
* continuous, numeric variables, e.g. age of the policyholder, age of the car
* spatial information: postal code (in Belgium) of the municipality where the policyholder resides.

More details in [Henckaerts et al. (2018, Scandinavian Actuarial Journal)](https://katrienantonio.github.io/portfolio/machine-learning) and [Henckaerts et al. (2020, North American Actuarial Journal)](https://katrienantonio.github.io/portfolio/machine-learning).

---

# Data sets used in this session - MTPL <img src="image/pipe.png" class="title-hex"> <img src="image/dplyr.png" class="title-hex"> <img src="image/ggplot2.png" class="title-hex">

You can load the data from a .R script in the course material:

```{r eval = FALSE}
# install.packages("rstudioapi")
dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(dir)
mtpl_orig <- read.table('./data/PC_data.txt', 
                                  header = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

If you work in an R notebook or R markdown file, you can also go for:
```{r eval = FALSE}
# install.packages("here")
library(here)
dir <- here::here()   
setwd(dir) 
mtpl_orig <- read.table('./data/PC_data.txt', 
                                  header = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

The last instruction transforms `mtpl_orig` into a `tibble`.

---

class: clear

These instructions are recommended because they .KULbginline[avoid referring to a specific working directory on your computer], e.g.

```{r eval = FALSE}
setwd("C:\\Users\\u0043788\\Dropbox\\Loss models course\\data") 
mtpl_orig <- read.table('PC_data.txt', header = TRUE)
```

--

If you organize your data analysis or project in a folder on your computer that holds all relevant files, then the above instructions allow you (and your colleagues) to get started right away. 

The only thing you need is an organized file structure.

--

The {rstudioapi} package is developed for RStudio. The {here} library is more general.

Do note that when working in a .Rmd, {here} will use the folder that markdown lives in as the working directory, but if you are working in a script (.R) the working directory is the top level of the project file.

---

class: clear
name: explore-data

First of all, we explore the structure of `mtpl_orig` with `str()`.

```{r echo=FALSE}
# install.packages("here")
library(here)
dir <- here::here()   
#setwd(dir) 
mtpl_orig <- read.table('./data/PC_data.txt', 
                                  header = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

```{r eval = TRUE}
str(mtpl_orig)
```

---

class: clear
name: explore-data

Or with `head()`...

```{r eval = TRUE}
head(mtpl_orig) %>% kable(format = 'html')
```



---

class: clear
name: data-sets-used

Note that the data `mtpl_orig` uses capitals for the variable names

```{r, out.width='35%'}
mtpl_orig %>% slice(1:3) %>% select(-LONG, -LAT) %>% kable(format = 'html')
```

We change this to lower case variables, and rename `exp` to `expo`.

```{r, eval = F}
mtpl <- mtpl_orig %>%
  # rename all columns 
  rename_all(function(.name) {
    .name %>% 
      # replace all names with the lowercase versions
      tolower 
    })
mtpl <- rename(mtpl, expo = exp)
```

Check `rename_all()` in {dplyr}.

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

To get warmed up, let's load the `mtpl` data and do some .KULbginline[basic investigations] into the variables. The idea is to get a feel for the data. 

Your starting point are the instructions in the R script on the course website.

.hi-pink[Q]: you will work through the following exploratory steps.

1. Calculate the empirical claim frequency, per unit of exposure. Then do the same per gender. What do you conclude?

2. Visualize the distribution of `nclaims` with a bar plot, use `geom_bar` as a layer in `ggplot`. Try different versions of this bar plot.

3. Visualize the distribution of `avg` with a density plot, use `geom_density` as a layer. Restrict the range of values displayed in the plot.

4. Visualize the distribution of `avg` with a histogram, use `geom_histogram`. Play with the `binwidth`.



]


---

class: clear
name: first-steps-MTPL

```{r prepare-mtpl, echo = F}
mtpl <- mtpl_orig %>%
  # rename all columns 
  rename_all(function(.name) {
    .name %>% 
      # replace all names with the lowercase versions
      tolower 
      # replace all spaces with underscores is also useful, with `str_replace(" ", "-")`
    })
mtpl <- rename(mtpl, expo = exp)
```

.pull-left[
```{r first-inspection-mtpl, eval = F}
dim(mtpl)
```

```{r , echo = F}
dim(mtpl)
```

```{r first-risk-calculations-mtpl-2, eval = F}
mtpl %>% summarize(emp_freq = 
                      sum(nclaims) / sum(expo))
```

```{r, echo = F}
mtpl %>% summarize(emp_freq = sum(nclaims) / sum(expo)) %>% kable(format = 'html')
```

```{r first-risk-calculations-mtpl-3, eval = F}
mtpl %>% 
  group_by(sex) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))
```

```{r, echo = F}
mtpl %>% 
  group_by(sex) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo)) %>% kable(format = "html") 
```
]

.pull-right[
```{r first-graphs-mtpl, eval = F}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), col = KULbg, 
              fill = KULbg, alpha = 0.5) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g
```

```{r , echo = F, out.width = '80%'}
KULbg <- "#116E8A"
g <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), col = KULbg, 
                               fill = KULbg, alpha = 0.5) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g
```

]

---

class: clear

.pull-left[
```{r first-graphs-mtpl-2, eval = F}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), col = KULbg, 
              fill = KULbg, alpha = 0.5) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g
```

```{r , echo = F, out.width = '80%'}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), col = KULbg, 
              fill = KULbg, alpha = 0.5) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g
```

]

.pull-right[
```{r, eval = F}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw()
g + geom_bar(aes(y = (..count..)/sum(..count..)), 
    col = KULbg, fill = KULbg, alpha = 0.5) + 
  labs(y = "Relative frequency") +
  ggtitle("MTPL - relative number of claims")
```

```{r, echo = F, out.width = '80%'}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw()
g + geom_bar(aes(y = (..count..)/sum(..count..)), 
             col = KULbg, fill = KULbg, alpha = 0.5) + 
  labs(y = "Relative frequency") +
  ggtitle("MTPL - relative number of claims")
```

]

---

class: clear

.pull-left[

With a density plot: mind the `filter(.)` instruction and the use of `xlim(.,.)`

```{r eval = FALSE}
g_dens <- mtpl %>% filter(avg > 0 & avg <= 81000) %>% 
          ggplot(aes(x = avg)) + 
          theme_bw() +
          geom_density(adjust = 3, col = KULbg, 
                       fill = KULbg, alpha = 0.5) + 
          xlim(0, 1e4) + 
          ylab("") + xlab("severity") +
    ggtitle("MTPL data - average amount per claim")
g_dens
```

]

.pull-right[

```{r, echo = F, out.width = '80%'}
g_dens <- mtpl %>% filter(avg > 0 & avg <= 81000) %>% ggplot(aes(x = avg)) + theme_bw() +
            geom_density(adjust = 3, col = KULbg, fill = KULbg, alpha = 0.5) + xlim(0, 1e4) + ylab("") + xlab("severity") +
            ggtitle("MTPL data - average amount per claim")
g_dens
```

]
---

class: clear

.pull-left[

With a histogram:

```{r eval = FALSE}
g <- mtpl %>% filter(avg > 0 & avg <= 81000) %>% 
     ggplot(aes(x = avg)) + 
      theme_bw() + xlab("severity") +
      geom_histogram(binwidth = 100, col = KULbg, 
            fill = KULbg, alpha = 0.5) + 
      xlim(0, 1e4) +
      labs(y = "Frequency histogram")
g
```

]

.pull-right[

```{r echo = FALSE, out.width= '80%'}
g <- mtpl %>% filter(avg > 0 & avg <= 81000) %>% ggplot(aes(x = avg)) + theme_bw() + xlab("severity") +
  geom_histogram(binwidth = 100, col = KULbg, 
                   fill = KULbg, alpha = 0.5) + xlim(0, 1e4) +
  labs(y = "Frequency histogram")
g
```

]
---

# Data sets used in this session - Secura Re losses

.pull-left[
Secura Belgian Re automobile claims from 1988 to 2001 data, gathered from several European insurance companies, .KULbginlin[exceeding 1 200 000 Euro]. 

The data were, among others, corrected for inflation, see the {ReIns} library.

```{r}
secura <- read.table(file="./data/SecuraRe.txt", 
                   header = TRUE, sep = "\t")
```

The density plots on the right show typical features of insurance loss data: 

- positive
- skewed to the right
- heavy right tail
- (possibly) subject to truncation and censoring.

]

.pull-right[
```{r, message = FALSE, echo = FALSE}
require(gridExtra)
grid.arrange(
  ggplot(secura, aes(Loss)) +
    theme_bw() +
    geom_density(col = KULbg, fill = KULbg, alpha = 0.5) +
    ggtitle('Secura Re - losses') + xlab('loss') + ylab(''),
  ggplot(secura, aes(log(Loss))) +
    theme_bw() +
    geom_density(col = KULbg, fill = KULbg, alpha = 0.5) +
    ggtitle('Secura Re  - losses, log-scale') + xlab('log(loss)') + ylab(''))
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will now use the instructions covered in the exploration of the `mtpl` data to picture the `secura` losses.

.hi-pink[Q]: you will work through the following exploratory steps.

1. Visualize the distribution of `Loss` in `secura` with a density plot, use `geom_density` as a layer. 

2. Do the same for `log(Loss)`.

3. Plot the `Loss` versus `Year`. Use the `geom_point` layer.


]

---

class: clear

The `secura` losses over time...

.pull-left[
```{r eval = FALSE}
ggplot(secura, aes(Year, Loss)) + theme_bw() + 
  geom_point(colour = KULbg, size = 2, alpha = 0.5) + 
  ggtitle('Secura Re - losses arriving over time')
```
]

.pull-right[
```{r echo = FALSE, out.width= '100%'}
ggplot(secura, aes(Year, Loss)) + theme_bw() + geom_point(colour = KULbg, size = 2, alpha = 0.5) + 
  ggtitle('Secura Re - losses arriving over time')
```
]

---

class: inverse, center, middle
name: frequency

# Fitting frequency models in R

---

name: freq-sev approach 

# The frequency - severity approach 

.pull-left[

<center>
<img src="image/freq-sev.png" width="600"/>
</center>

.KULbginline[Assumptions:] 

* claim severity and claim frequency are independent

* we ignore any additional covariates (cfr. workshop last year).


]

.pull-right[

Each record $i$ in the data set registers
  
* .KULbginline[exposure], $e_i$, fraction of the year in which the contract was active 
  
* .KULbginline[number of claims], $N_i$, observed claim count in the period of exposure
  
* .KULbginline[average claim size] per reported claim (or: severity), $S_i$, total losses in the period of exposure divided by the number of claims, if $N_i>0$.


]

---

name: freq-sev approach 

# The (a, b, 0) class

.pull-left[

Let $N$ be a count or frequency random variable, with probability function (pf) $Pr(N = k) = p_k$.

The $(a,b,0)$-class of frequency distributions is a two-parameter class of distributions that satisfy

$$k \cdot \frac{p_k}{p_{k-1}} = a \cdot k + b \quad k \geq 1.$$

The probability $p_0$ then follows from $\sum_{k=0}^{+\infty} p_k = 1$.

Members of this class are the:

* Poisson
* Binomial
* Negative Binomial
* Geometric distribution.

]

.pull-right[

<center>
<img src="image/LossModels.jpg" width="300"/>
</center>


]

---

class: clear


Relevant claim frequency distributions in the $(a,b,0)$ class: 

<br/>

Member           |   slope $a$ | Probability function $p_k$ |  Parameter(s)  | Moments
:-------------------------|:-------------------------|:-------------------------|:-------------------------
Binomial | $a < 0$ | $\frac{n!}{k!(n-k)!} \cdot p^k \cdot (1-p)^k$  |  $p \in (0, 1)$ |  $E(N) = n \cdot p$
         |    |                         | $n \in \mathbb{N}$  | $\text{Var}(N) = n \cdot p \cdot (1-p)$ 
Poisson | $a = 0$ | $e^{-\lambda} \cdot \frac{\lambda^k}{k!}$  |  $\lambda \in (0, \infty)$  |    $E(N) = \lambda$
   |    |   |   | $\text{Var}(N) = \lambda$ 
Negative Binomial | $a > 0$ | $\frac{\Gamma(r+k)}{k! \Gamma(r)} \cdot \left( \frac{r}{r+\mu}\right)^r \cdot \left( \frac{\mu}{r+\mu} \right)^k$ | $\mu \in (0, \infty)$ | $E(N) = \mu$
       |    |   | $r \in (0, \infty)$   |   $\text{Var}(N) = \mu + \frac{\mu^2}{r}$ 


---

# The (a, b, 0) class: a visual check

.pull-left[

To check the $(a,b,0)$ relation for `mtpl$nclaims` we start from the empirical count distribution

```{r, eval = FALSE}
mtpl$nclaims %>% table %>% prop.table
```

We store the empirical probabilities $\hat{p}_k$ in a vector

```{r}
empirical <- mtpl$nclaims %>% 
  table %>% prop.table %>% as.numeric
```

and the $k \cdot \frac{p_k}{p_{k-1}}$

```{r}
k <- 1:(length(empirical) - 1)
ab0_relation <- empirical[k+1] / empirical[k] * k  
```

```{r}
ab0_data <- tibble(k = k, ab0_rel = ab0_relation)
```

]

.pull-right[

The positive slope when fitting $a \cdot k + b$ indicates a NegBin distribution for `mtpl$nclaims`, see 

```{r eval = FALSE}
ab0_data %>% lm(ab0_rel ~ k, data = .)
```

and visually

```{r, echo = FALSE, fig.height=4, fig.width = 8}
empirical <- mtpl$nclaims %>% 
  table %>% prop.table %>% as.numeric

k <- 1:(length(empirical)-1)
ab0_relation <- empirical[k+1] / empirical[k] * k  

ggplot(data.frame(k = k, relation = ab0_relation), 
       aes(x = k, y = relation)) +
  theme_bw() + ylab('frac') +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

]

---

# Empirical mean and variance of the claim frequency

.pull-left[
Let's explore different ways to calculate the .KULbginline[empirical mean claim frequency].

Inspect the following instructions and list pros and cons:

```{r eval = FALSE}
mean(mtpl$nclaims)
sum(mtpl$nclaims)/sum(mtpl$expo)
weighted.mean(mtpl$nclaims/mtpl$expo, mtpl$expo)
mtpl %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo)) 
```

```{r echo = FALSE}
mean(mtpl$nclaims)
sum(mtpl$nclaims)/sum(mtpl$expo)
weighted.mean(mtpl$nclaims/mtpl$expo, mtpl$expo)
mtpl %>% summarize(emp_freq = sum(nclaims) / sum(expo)) %>% kable(format = 'html')
```
]

.pull-right[

What about the .KULbginline[empirical variance]? .KULbginline[Overdispersion]!

```{r}
m <- sum(mtpl$nclaims)/sum(mtpl$expo)
m
var <- sum((mtpl$nclaims - m * mtpl$expo)^2)/
                sum(mtpl$expo)
var
```

Here we use the expression for the variance: 

$$\text{Var}(X) = \text{E}[(X-\text{E}X)^2] $$
We empirically calculate the first expected value by taking exposure into account. 

Moreover, we compare each realization of `nclaims` with its expected value, i.e. `m * expo` where `m` is the empirical mean claim frequency. 

]

---

# Maximum likelihood estimation (MLE)

.pull-left[

Fit a distribution to the data by .KULbginline[maximizing the likelihood] over the unknown parameter vector $\theta$

$$ \mathcal{L}(\theta) = \prod_{i} Pr(N_i = n_i \mid \theta).$$

In practice, we minimize the negative log-likelihood
$$L(\theta) = - \sum_{i} \log(Pr(N_i = n_i \mid \theta)).$$

If exposure is available, we'll typically incorporate the exposure measure in the mean of the distribution
$$L(\theta) = - \sum_{i} \log(Pr(N_i = n_i \mid \theta,\ e_i)).$$

]

.pull-right[

Let's make this more concrete for the Poisson distribution:

$$N_i \sim \texttt{Poi}(e_i \cdot \lambda).$$
The corresponding log-likelihood becomes: 

$$\sum_i n_i \cdot \log(e_i \cdot \lambda) - e_i \cdot \lambda - n_i! $$
How about .KULbginline[fitting a parametric count distribution] to data in R?

]

---

# Maximum likelihood estimation (MLE) in R

.pull-left[

A (naive) first solution uses `fitdistr(.)` from the {MASS} library

```{r}
library(MASS)
fitdistr(mtpl$nclaims, "poisson")
```

Which number do you recognize here for $\hat{\lambda}$?

Alternatively, we can use the `glm(.)` function in a smart way:

```{r eval=FALSE}
freq_glm_poi <- `glm`(nclaims ~ 1, 
                  `family = poisson(link = "log")`, 
                  `data = mtpl`) 
```


]

.pull-right[

Fit a .KULbginline[Poisson GLM], with .KULbginline[logarithmic link] function.

This implies: 

$\color{#FFA500}{Y}$ ~ Poisson, with only an intercept in the linear predictor

$$\begin{eqnarray*}
    \log(E[\color{#FFA500}{Y}]) &=& \color{#20B2AA}{\beta_0},
\end{eqnarray*}$$

or, 

$$E[\color{#FFA500}{Y}] = \lambda = \exp{(\color{#20B2AA}{\beta_0})}.$$

Fit this model on `data = mtpl`. 

]

---

class: clear

.pull-left[

```{r eval=FALSE}
freq_glm_poi <- glm(`nclaims ~ 1`, `offset = log(expo)`, 
                  family = poisson(link = "log"), 
                  data = mtpl)
freq_glm_poi %>% broom::tidy()
```

```{r echo=FALSE}
freq_glm_poi <- glm(nclaims ~ 1, offset = log(expo), 
                  family = poisson(link = "log"), 
                  data = mtpl)
freq_glm_poi %>% broom::tidy() %>% kable(format = 'html')
```

What is your estimate for the expected annual claim frequency?

]

.pull-right[
  
Use `nclaims` as $\color{#FFA500}{Y}$. 

Use only an intercept in the linear predictor `~ 1`.

Include `log(expo)` as an offset term in the linear predictor.

Then, 

$$\begin{eqnarray*}
\log{(\texttt{expo})}+\beta_0. \end{eqnarray*}$$

Put otherwise, 

$$\begin{eqnarray*}
E[\color{#FFA500}{Y}] = \texttt{expo} \cdot \exp{(\beta_0)} \end{eqnarray*},$$
where $\texttt{expo}$ refers to `expo` the exposure variable.
]

---

# Maximum likelihood estimation (MLE) in R (continued)

.pull-left[

.KULbginline[Writing a function] for the negative log-likelihood is another option. 

The general framework for a one-parameter distribution then becomes

```{r eval=FALSE}
neg_loglik_distr <- function(par, freq, expo){
  -sum(ddistr(freq, par, log = T))
}
nlm(neg_loglik_distr, 1, hessian = TRUE, 
    freq = mtpl$nclaims, expo = mtpl$expo)
```

Tweaking the general framework to the Poisson setting then becomes ...

]

.pull-right[

```{r}
neg_loglik_pois <- function(par, freq, expo){
  lambda <- expo*exp(par)
  -sum(dpois(freq, lambda, log = T))
}
sol_poi <- nlm(neg_loglik_pois, 1, hessian = TRUE, 
               freq = mtpl$nclaims, expo = mtpl$expo)
```

Inspect the results

```{r}
exp(sol_poi$estimate)
sqrt(diag(solve(sol_poi$hessian))) 
sol_poi$minimum
```

Do you recognize these numbers?

]

---

class: clear

.pull-left[

We now focus on fitting the Negative Binomial distribution to the `mtpl$nclaims` frequency data, using `glm.nb()` from the {MASS} library.

```{r eval=FALSE}
library(MASS)
freq_glm_nb <- glm.nb(`nclaims ~ 1 +` 
                         `offset(log(expo))`, 
                  link = log,
                  data = mtpl)
freq_glm_nb %>% broom::tidy()
```

```{r echo=FALSE}
freq_glm_nb <- glm.nb(nclaims ~ 1 + offset(log(expo)), 
                  link = log,
                  data = mtpl)
freq_glm_nb %>% broom::tidy() %>% kable(format = 'html')
```

The additional parameter $r$ is estimated as 
(point estimate and standard error)

```{r echo=FALSE}
freq_glm_nb$theta
freq_glm_nb$SE.theta
```


]


.pull-right[

Inspect the model output via

```{r out.height="35%"}
summary(freq_glm_nb)
```


]


---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

.hi-pink[Q]: you will work through the following steps

1. Starting from the handcrafted Poisson likelihood and its numerical optimization, can you write similar instructions for the Negative Binomial?

2. Can you match the estimates obtained with your routine with those of the `glm.nb()` call?


]

---

class: clear

.pull-left[

Let's now focus on the handcrafted Negative Binomial likelihood optimization:

```{r}
neg_loglik_nb <- function(par, freq, expo){
  mu <- expo*exp(par[1])
  r <- exp(par[2])
  -sum(dnbinom(freq, size = r, mu = mu, log = TRUE))
}
sol_nb <- nlm(neg_loglik_nb, c(1, 1), hessian = TRUE, 
              freq = mtpl$nclaims, expo = mtpl$expo)
```

Inspect the resulting estimates

```{r}
sol_nb$estimate
```


]

.pull-right[

We inspect the results

```{r}
exp(sol_nb$estimate)
sqrt(diag(solve(sol_nb$hessian))) 
sol_nb$minimum
```

The variance estimated by this Negative Binomial model is then

```{r}
exp(sol_nb$estimate[1]) + 
(exp(sol_nb$estimate[1])^2)/(exp(sol_nb$estimate[2]))
```

which is very close to the empirical variance!




]

---

# Model selection and evaluation tools

The .KULbginline[AIC] can be used to compare models

$$AIC = 2 \cdot \#\text{param} - 2 \cdot \text{log-likelihood}.$$

```{r}
AIC_poi <- 2*length(sol_poi$estimate) + 2*sol_poi$minimum
AIC_nb <- 2*length(sol_nb$estimate) + 2*sol_nb$minimum
c(AIC_nb = AIC_nb, AIC_poi = AIC_poi)
```

Smaller is better, thus preferred distribution is the Negative Binomial based on AIC!

---

class: clear

Next to this, we compare the .KULbginline[observed and fitted claim count] distribution. 

For example, with the fitted Negative Binomial distribution

```{r}
observed_count = rep(0, 5)
model_based_count = rep(0, 5)

for(i in 1:5) {
  observed_count[i] = sum(mtpl$nclaims == i-1)
  model_based_count[i] = sum(dnbinom(i-1, size = freq_glm_nb$theta, mu = fitted(freq_glm_nb)))
}

data.frame(frequency = 0:4,
           observed_count = observed_count,
           model_based_count = round(model_based_count))

```

How would you explain the code (in your own words)? Can you adjust the code to the Poisson distribution?

---

# Excess zeroes in claim count data

Standard count distributions have often difficulty to capture the large number of zeroes in claim frequency data.

We propose two strategies to model an .KULbginline[excessive number of zeroes].

With a .KULbginline[Zero-Inflated (ZI)] distribution:

$$ 
P(N^{ZI} = k) = 
\begin{cases}
\pi + (1-\pi) \cdot P(N=0) & k = 0 \\
(1-\pi) \cdot P(N = k) & k \gt 0.
\end{cases}
$$
Zero-inflated count models are two-component mixture models combining a point mass at zero with a proper count distribution. Thus, there are two sources of zeroes: zeroes may come from both the point mass and from the count component. 

With a .KULbginline[Hurdle] distribution: 

$$ 
P(N^{hurdle} = k) = 
\begin{cases}
\pi & k = 0 \\
(1-\pi)\cdot \frac{P(N=k)}{1-P(N = 0)} & k \gt 0.
\end{cases}
$$
How can we fit these distributions to given data with R?

---

class: clear

.pull-left[
The [library {pscl}](https://github.com/atahk/pscl) enables maximum likelihood estimation of zero-inflated and hurdle models for count data.

```{r}
library(pscl)
f_ZIP <- zeroinfl(nclaims ~ 1, offset = log(expo), 
                  dist = "poisson", data = mtpl)
summary(f_ZIP)
```


]

.pull-right[

The `dist = .` available in `zeroinfl(.)` are the Poisson, Negative Binomial and the geometric distribution.

The `offset = .` is an a priori known component to be included in the linear predictor of the count model (as we did before).

Usually the count model is a Poisson or negative binomial regression (with log link). 

A binary model is used that captures the probability of zero inflation. In the simplest case only with an intercept but potentially containing regressors. 
For this zero-inflation model, a binomial model with different links can be used, typically logit or probit.

]

---

class: clear

More detailed inspection of the output stored in `f_ZIP`

```{r}
f_ZIP$coefficients
(f_ZIP_lambda <- exp(as.numeric(f_ZIP$coefficients[1])))
(f_ZIP_pi <- exp(as.numeric(f_ZIP$coefficients[2]))/(1+exp(as.numeric(f_ZIP$coefficients[2]))))
(AIC_ZIP <- 2*length(f_ZIP$coefficients) - 2*f_ZIP$loglik)
```

The mean and variance (for `expo == 1`) as captured by the ZIP

```{r}
(ZIP_mean <- (1-f_ZIP_pi)*f_ZIP_lambda)
(ZIP_var <- (1-f_ZIP_pi)*f_ZIP_lambda*(1+f_ZIP_pi*f_ZIP_lambda))
```

---

class: clear

A handcrafted MLE with the .KULbginline[ZIP distribution] can be put together as

```{r}
neg_loglik_ZIP <- function(par, freq, expo){
  lambda <- expo*exp(par[1])
  p <- exp(par[2])/(1+exp(par[2]))
  
  -sum((freq == 0) * (log(p + (1-p)*dpois(0, lambda, log = FALSE)))) - 
    sum((freq != 0) * (log((1-p)) + dpois(freq, lambda, log = TRUE)))
}

sol_ZIP <- nlm(neg_loglik_ZIP, c(1, 1), hessian = TRUE, 
              freq = mtpl$nclaims, expo = mtpl$expo)
sol_ZIP$estimate
```

and their corresponding standard errors

```{r}
(sol_ZIP_se <- sqrt(diag(solve(sol_ZIP$hessian))))
```

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

.hi-pink[Q]: as a final challenge

1. Adjust the code of the ZIP to fit the hurdle Poisson model, use `pscl::hurdle()`.

2. Starting from the handcrafted ZIP likelihood, write code for the hurdle Poisson model.


]

---

class: inverse, center, middle
name: severity

# Fitting severity models in R

---

name: freq-sev approach 

# From simple to complex parametric distributions

.pull-left[

Famous examples of .KULbginline[1 and 2 parameter distributions] used in loss modelling: 

- Exponential
- Lognormal
- Gamma and Inverse Gaussian.

Some less straightforward .KULbginline[more flexible parametric distributions]: 

- Burr (3 parameters)
- GB2 (4 parameters).

Typically challenging to fit, picking meaningful starting values in numerical optimization routines matters!


]

.pull-right[

<center>
<img src="image/LossModels.jpg" width="300"/>
</center>


]

---

class: clear

<br/>

<br/>

Distribution           |   Density | Mean  |  R  | 
:-------------------------|:-------------------------|:-------------------------|
Exponential | $f(y) = \displaystyle \lambda e^{-\lambda y}$ | $\text{E}[Y] = \displaystyle 1/\lambda$  | `dexp` ... 
Gamma | $f(y) = \displaystyle \frac{1}{\Gamma(\alpha)}\beta^{\alpha}y^{\alpha-1}e^{-\beta y}$ | $\text{E}[Y] = \displaystyle \alpha/\beta$  | `dgamma` ...
Inverse Gaussian | $f(y) = \displaystyle \left(\frac{\lambda}{2\pi y^3}\right)^{1/2}\exp{\left[\frac{-\lambda(y-\mu)^2}{2\mu^2y}\right]}$ | $\text{E}[Y] = \mu$  | `dinvgaus` package {statmod}
Lognormal | $f(y) = \displaystyle \frac{1}{\sqrt{2\pi} \sigma y}\exp{\left[-\frac{1}{2} \left(\frac{\log{y}-\mu}{\sigma}\right)^2\right]}$ | $\text{E}[Y] = \displaystyle \exp{\left(\mu+\frac{1}{2}\sigma^2\right)}$ | `dlnorm` ...

The {actuar} package has some functions related to the Burr and GB2 distributions, e.g. `dburr` and `dgenbeta`.

Mind the .KULbginline[different parametrizations] of these distributions!

---

# Be aware of truncation!

In `secura` only losses .KULbginline[above 1.2M EUR] are registered. 

We can picture this as follows: 

<center>
<img src="image/trunc_pic.png" width="500"/>
</center>

Lower/left truncation: deductible, reinsurance.

Upper/right truncation: earthquake magnitudes.

How would you tackle this left-truncation in `secura` when fitting a loss distribution to the data?

---

# Be aware of truncation and censoring!

<center>
<img src="image/cens_trunc_pic.png" width="900"/>
</center>

---

# Global fitting distributions

.pull-left[
How to find/to construct a .KULbginline[global fit] of insurance losses?

That's .KULbginline[hard], because: 

- different behaviour of attritional and large losses (body vs tail).

Typically, no standard parametric distribution provides suitable global fit: 

- Lognormal, Weibull, etc. underestimate tail risk
- Pareto, Generalised Pareto Distribution (GPD), etc. can only be used for large losses
- complex distributions (e.g. GB2).

]

.pull-right[

.KULbginline[Splicing (or composite modeling)] offers a useful strategy to construct a global fit.

Intuitively: 

- different components on different intervals of losses
- glue these components together into a well-defined density.

This technique is popular e.g. in modelling operational risk data. 

]

---

# Splicing: a body-tail example

.pull-left[
Combine two distributions (one for the .KULbginline[body] and one for the .KULbginline[tail]) in a splicing model:

$$f(x;\boldsymbol{\Theta}) = \begin{cases}
	0 & \quad \text{if }x< t^l \\
	\pi \frac{f_1^*(x;t,\boldsymbol{\Theta}_1)}{F_1^*(t;\boldsymbol{\Theta}_1)-F_1^*(t^l;\boldsymbol{\Theta}_1)} & \quad \text{if }t^l \leq x\leq t \\
	(1-\pi) \frac{f_2^*(x;t,\boldsymbol{\Theta}_2)}{1-F_2^*(t;\boldsymbol{\Theta}_1)} &\quad \text{if } x > t,
	\end{cases}$$

where $t^l$ is the .KULbginline[left-truncation point] and $t$ the .KULbginline[split point] (body vs. tail).

Questions for you: 

- which in the above should be estimated from the data?
- why the conditional densities, 
  constructed from original pdf's $f_1^*(.)$ and $f_2^*(.)$ on the positive real line?

]

.pull-right[

Some examples of body-tail fits: 

Body           |   Tail 
:-------------------------|:-------------------------|
		Exponential			|	Pareto 
		Lognormal			|	Pareto
		Weibull				|	Pareto
		Mixture of two exponentials	|	Generalised Pareto 

]

---

# A body-tail fit for the Secura Re losses

Let's combine an .KULbginline[exponential] (body - below threshold $t$) and a .KULbginline[Pareto] (tail - above $t$)

$$\begin{eqnarray*}
f(x) &=& \begin{cases}
	0 & x \leq t^l \\
	\pi \cdot \color{blue}{\frac{f_1^{\star}(x)}{F_1^{\star}(t)-F_1^{\star}(t^l)}} & t^l \leq x \leq t \\
(1-\pi) \cdot \color{orange}{\frac{f_2^{\star}(x)}{1-F_2^{\star}(t)}} & x > t \end{cases},
\end{eqnarray*}$$

with 

- $f_1^{\star}$ and $F_1^{\star}$ the PDF and CDF of an exponential distribution

$$\begin{eqnarray*}
\frac{f_1^{\star}(x)}{F_1^{\star}(t)-F_1^{\star}(t^l)} &=& \frac{\lambda \exp\{-\lambda x\}}{\exp\{-\lambda \cdot t^l\}-\exp\{-\lambda \cdot t\}}=\frac{\lambda \exp\{-\lambda (x-t^l)\}}{1-\exp\{-\lambda(t-t^l)\}},
\end{eqnarray*}$$

- $f_2^{\star}$ and $F_2^{\star}$ the PDF and CDF of a Pareto distribution with unit scale

$$\begin{eqnarray*}
\frac{f_2^{\star}(x)}{1-F_2^{\star}(t)} &=& \frac{\frac1{\gamma} x^{-\frac1{\gamma}-1}}{t^{-\frac1{\gamma}}}.
\end{eqnarray*}$$

---

# Choice of threshold

.pull-left[

Techniques from .KULbginline[extreme value theory (EVT)] are necessary to estimate .KULbginlin[split point] above which a Pareto distribution is plausible.

A first visual tool is the .KULbginline[mean excess plot]: 

- the mean excess function or mean residual life function is

$$e(t) = E(X-t|X>t)$$

- empirically, using a sample $(x_1,\ldots,x_n)$

$$\hat{e}_n(t) = \frac{\sum_{i=1}^n x_i \boldsymbol{1}_{(t,\infty)}(x_i)}{\sum_{i=1}^n \boldsymbol{1}_{(t,\infty)}(x_i)}-t.$$
- the mean excess plot evaluates the above at values $t=x_{n-k,n}$, the $(k+1)$th largest observation.

]

.pull-right[

The {ReIns} package implements many useful functions from this book:


<center>
<img src="image/ReIns.jpg" width="300"/>
</center>

]


---

class: clear

.pull-left[

Via {ReIns} we can easily plot the mean excess function: 

```{r eval=FALSE}
library(ReIns)
ReIns::MeanExcess(secura$Loss)
```

By default, the mean excess scores are plotted as a function of the data `k = FALSE`. 

You can also plot these as function of the tail parameter `k = TRUE`.

The mean excess function of an EXP distribution is constant, then: 

- .KULbginline[HTE] ('Heavier than exponential'), mean excess function .KULbginline[ultimately increases]

- .KULbginline[LTE] ('Lighter than exponential'), when mean excess function .KULbginline[ultimately decreases].


]

.pull-right[

```{r echo=FALSE}
library(ReIns)
ReIns::MeanExcess(secura$Loss)
#abline(v = 2.6 * 10^6, lwd = 2, lty = 2)
```

]

---

class: clear

.pull-left[

A second visual tool is a QQplot, e.g. the .KULbginline[Exponential QQplot]:

- recall $F_{\lambda}(x) = 1-\exp{(-\lambda x)}$ with $x>0$

- then $Q_{\lambda}(p) = -\frac{1}{\lambda} \log{(1-p)}$ for $p \in (0,1)$

- use $p_{i,n} := i/(n+1)$ and plot

$$(-\log{(1-p_{i,n})},x_{i,n}).$$

]

.pull-right[


```{r eval=FALSE}
ReIns::ExpQQ(secura$Loss)
```


```{r echo=FALSE, out.width='80%'}
ReIns::ExpQQ(secura$Loss)
```


]

---

class: clear

.pull-left[

A second visual tool is a QQplot, e.g. the .KULbginline[Pareto QQplot]:

- recall $F_{\gamma}(x) = 1-x^{-1/\gamma}$ with $x>0$

- then $-\log{(1-p)} = \frac{1}{\gamma} \log x$ for $p \in (0,1)$

- use $p_{i,n} := i/(n+1)$ and plot

$$(-\log{(1-p_{i,n})},\log x_{i,n}).$$

A distribution is of Pareto-type when the Pareto QQplot is ultimately linear near the largest observations.

]

.pull-right[


```{r eval=FALSE}
ReIns::ParetoQQ(secura$Loss)
```


```{r echo=FALSE, out.width='80%'}
ReIns::ParetoQQ(secura$Loss)
```


]


---

# Choice of threshold - the Hill estimator

.pull-left[

We estimate the slope of the Pareto QQplot to the right of the reference point

$$(-\log{(\frac{k+1}{n+1})},\log x_{n-k,n}).$$
This slope can be expressed as

$$H_{k,n} = \frac{1}{k} \sum_{j=1}^k \log X_{n-j+1,n} - \log X_{n-k,n}.$$

When the data has a Pareto tail the Hill plot becomes linear.

But mind the bias and variance trade off!

]

.pull-right[

```{r eval=FALSE}
H <- ReIns::Hill(secura$Loss, k = FALSE, 
                 lwd = 2, plot = TRUE, main = "")
```

```{r echo=FALSE, out.width='80%'}
H <- ReIns::Hill(secura$Loss, k = FALSE, lwd = 2, plot = TRUE, main = "")
```

]

---

# Choice of threshold - the Hill estimator

.pull-left[

We estimate the slope of the Pareto QQplot to the right of the reference point

$$(-\log{(\frac{k+1}{n+1})},\log x_{n-k,n}).$$
This slope can be expressed as

$$H_{k,n} = \frac{1}{k} \sum_{j=1}^k \log X_{n-j+1,n} - \log X_{n-k,n}.$$

When the data has a Pareto tail the Hill plot becomes linear.

But mind the bias and variance trade off!

]

.pull-right[

```{r eval=FALSE}
H <- ReIns::Hill(secura$Loss, k = TRUE, 
                 lwd = 2, plot = TRUE, main = "")
```

```{r echo=FALSE, out.width='80%'}
H <- ReIns::Hill(secura$Loss, k = TRUE, lwd = 2, plot = TRUE, main = "")
```

]


---

class: clear

.pull-left[

The Hill plot shows many possible estimates for $\gamma$ in the Pareto distribution $F(x)=1-x^{-1/\gamma}$ that we want to use for the tail of the distribution.

.KULbginline[How to pick the split point?]

We consider the Asymptotic Mean Squared Error of $H_{k,n}$ (i.e. variance + squared bias).

We plot

$$(k,\widehat{\text{AMSE}}(H_{k,n}));k=1,\ldots, n-1\}$$

and pick the $k$ that minimizes the AMSE.

]


.pull-right[

```{r eval=FALSE}
H <- ReIns::Hill(secura$Loss, lwd = 2, plot = TRUE, 
                 main = "")
kopt <- ReIns::Hill.kopt(secura$Loss, 
                         plot = FALSE)$kopt
abline(v = kopt, lwd = 2, lty = 2)
```

```{r echo=FALSE, out.width='80%'}
H <- ReIns::Hill(secura$Loss, lwd = 2, plot = TRUE, main = "")
kopt <- ReIns::Hill.kopt(secura$Loss, plot = FALSE)$kopt
abline(v = kopt, lwd = 2, lty = 2)
```

]

---

class: clear

.pull-left[

Using these insights from EVT we arrive at a .KULbginline[final choice] for the split point or threshold $t$.

We pick the optimal $k$ by minimizing AMSE of the Hill estimator.

Then, the corresponding split point or threshold $t := X_{n-k,n}$, the $(k+1)$th largest observation in the sample.

This reasoning also gives $\hat{\gamma}$ via the corresponding $H_{k,n}$. 

]

.pull-right[

```{r}
# sample size
n <- length(secura$Loss)
n

# Hill estimator
kopt <- Hill.kopt(secura$Loss, plot = FALSE)$kopt
kopt

# chosen threshold
threshold <- sort(secura$Loss)[n - kopt]
threshold

# estimate for gamma using Hill estimator
gamma <- H$gamma[kopt]
gamma
```


]

---

# Exponential - Pareto splicing model 

We are now ready to fit the .KULbginline[body-tail] model proposed earlier on:

$$\begin{eqnarray*}
f(x) &=& \begin{cases}
	0 & x \leq t^l \\
	\pi \cdot \color{blue}{\frac{f_1^{\star}(x)}{F_1^{\star}(t)-F_1^{\star}(t^l)}} & t^l \leq x \leq t \\
(1-\pi) \cdot \color{orange}{\frac{f_2^{\star}(x)}{1-F_2^{\star}(t)}} & x > t \end{cases},
\end{eqnarray*}$$

with 

- $f_1^{\star}$ and $F_1^{\star}$ the PDF and CDF of an exponential distribution

$$\begin{eqnarray*}
\frac{f_1^{\star}(x)}{F_1^{\star}(t)-F_1^{\star}(t^l)} &=& \frac{\lambda \exp\{-\lambda x\}}{\exp\{-\lambda \cdot t^l\}-\exp\{-\lambda \cdot t\}}=\frac{\lambda \exp\{-\lambda (x-t^l)\}}{1-\exp\{-\lambda(t-t^l)\}},
\end{eqnarray*}$$

- $f_2^{\star}$ and $F_2^{\star}$ the PDF and CDF of a Pareto distribution with unit scale

$$\begin{eqnarray*}
\frac{f_2^{\star}(x)}{1-F_2^{\star}(t)} &=& \frac{\frac1{\gamma} x^{-\frac1{\gamma}-1}}{t^{-\frac1{\gamma}}}.
\end{eqnarray*}$$

---

class: clear

In fact, we now have meaningful choices/estimates for:

- $t^l$ left truncation point at 1.2M EUR

- $t$ the split point estimated at `threshold`
```{r}
threshold
```

- the Hill estimator in `gamma`
```{r}
gamma
```

- the fraction of losses below the split point
```{r}
(p <- sum(secura$Loss <= threshold)/n)
```
or 
```{r}
(n-kopt)/n
```

---

class:clear


```{r}
sh <- 1200000 # shift
I <- secura$Loss <= threshold # indicator

neg.loglik <- function(par) {
	lambda <- exp(par[1]); alpha <- exp(par[2]) 
	# likelihood
	L <- I * (n-kopt)/n * lambda * exp(-lambda*(secura$Loss-sh))/(1 - exp(-lambda*(threshold-sh))) + 
		(1-I) * kopt/n * alpha * (secura$Loss)^(-alpha-1)/(threshold)^(-alpha)
	# negative log-likelihood
	-sum(log(L)) 
}

m1 <- mean(secura$Loss)
par.init <- c(1/(m1-sh), 1/m1)
oo <- nlm(neg.loglik, log(par.init))
(lambda <- exp(oo$estimate[1])) 
(alpha <- exp(oo$estimate[2]))
(gamma <- 1/alpha)
```

---

class: clear

By working out the integrals starting from the spliced density, we find:

for $t^l=1,200,000\leq x \leq t$:

$$\begin{eqnarray*}
F(x) &=& \pi \cdot \frac{1-\exp\{-\lambda(x-\text{1,200,000})\}}{1-\exp\{-\lambda(t-\text{1,200,000})\}}.
\end{eqnarray*}$$

and for $x>t$ we find 

$$F(x)=1-(1-\pi) \left( \frac{x}{t} \right)^{-\frac1{\gamma}}.$$

Note the different expressions for the CDF for the different ranges of $x$ values!

---

class: clear

Following function can be used the compute the .KULbginline[CDF of the spliced distribution] (try the integrals!)

```{r} 
# CDF for Exp-Pa splicing model
ExpPa_cdf <- function(x, sh, threshold, lambda, gamma) {
  
  p <- numeric(length(x))
  
  p[x <= threshold] <- (n - kopt) / n * pexp(x[x <= threshold] - sh, rate = lambda) / 
                                        pexp(threshold - sh, rate = lambda)
  
  p[x > threshold] <- 1 - kopt / n * (x[x > threshold] / threshold) ^ (-1/gamma)
  
  return(p)
}
```

---

class: clear

To assess the GoF of the fitted spliced distribution, we plot the empirical CDF together with the fitted CDF.

```{r echo=FALSE, out.width='40%'} 
# Plot empirical CDF and fitted CDF
plot(ecdf(secura$Loss), do.points = FALSE, xlab = "CDF", xlim = c(sh, 7 * 10^6), lwd=2)
lines(sort(secura$Loss), ExpPa_cdf(sort(secura$Loss), sh, threshold, lambda, gamma), lty = 2, lwd=2)
legend("bottomright", c("Empirical CDF", "Fitted CDF"), lty = c(1, 2), lwd = 2)
```

---

# A mixture of Erlangs for the body

.pull-left[

What if it is not straightforward to find a suitable distribution for the .KULbginline[body of the data]?

.KULbginline[Mixtures of Erlangs]: 

- versatile class of distributions, i.e. dense in the space of positive, continuous distributions
- mathematically tractable
- fitting procedure in {ReIns} package.

But, a ME distribution has an .KULbginline[exponential tail], thus no heavy tails! 

More details in [Verbelen et al. (2015, ASTIN Bulletin)](https://katrienantonio.github.io/portfolio/loss-models).

]

.pull-right[

The pdf of a mixture of Erlangs:

$$\begin{align*}
						f_X(x ; \boldsymbol{\alpha}, \boldsymbol{r}, \theta) &= \sum_{j=1}^{M} \alpha_j f_E(x;r_j, \theta) \\
						&= \sum_{j=1}^{M} \alpha_j \frac{x^{r_j-1} 	e^{-x/\theta}}{\theta^{r_j}(r_j-1)!},
\end{align*}$$

with number of Erlangs $M$, mixing weights $\boldsymbol{\alpha}$, common scale $\theta$ and positive integer shape parameters $(r_1,\ldots, r_M)$.

The mixing weights should satisfy $\alpha_j>0$ and $\sum_j \alpha_j = 1$.

Question for you: the Erlang distribution may remind you of a better known distribution among actuaries. Which one?

]


---

class: clear

.pull-left[
```{r}
# Fit ME model using internal function from 
# ReIns package
MEfit_sec <- ReIns:::.ME_tune(secura$Loss, 
                              trunclower = sh, 
                              criterium = "BIC", 
                              M = 10, 
                              s = 1:40)$best_model

# Fitted parameters
MEfit_sec$alpha
MEfit_sec$shape
MEfit_sec$theta
```
]

.pull-right[
```{r echo=FALSE, out.width='80%'}
# Histogram
truehist(secura$Loss, nbins = 40, ylim = c(0, 8e-07), xlab = "Claim size", ylab = "Density")
curve(ReIns:::.ME_density(x, theta = MEfit_sec$theta, shape = MEfit_sec$shape, 
                          alpha = MEfit_sec$alpha, trunclower = sh), 
      from = 10^6, to = 8 * 10^06, n = 10000, add = TRUE, col = "red", lwd = 2)
legend('topright', legend = c("ME Density", "Observed Relative Frequency"),
       col = c("red", "cyan"), pch = c(NA,15), pt.cex = 2, lty = c(1,NA), lwd = c(2,NA))
```
]

---

class: clear

.pull-left[

```{r eval=FALSE}
ME_VaR <- Vectorize(ReIns:::.ME_VaR, 
                    vectorize.args = "p")

# QQ-plot
plot(ME_VaR((1:n) / (n+1), theta = MEfit_sec$theta, 
            shape = MEfit_sec$shape, 
            alpha = MEfit_sec$alpha, 
            trunclower = sh), 
     sort(secura$Loss), 
     main = "ME QQ-plot", 
     xlab = "Fitted quantiles", 
     ylab = "Empirical quantiles")
abline(0, 1)
```

]

.pull-right[

```{r echo=FALSE, out.width='80%'}
ME_VaR <- Vectorize(ReIns:::.ME_VaR, 
                    vectorize.args = "p")

# QQ-plot
plot(ME_VaR((1:n) / (n+1), theta = MEfit_sec$theta, 
            shape = MEfit_sec$shape, 
            alpha = MEfit_sec$alpha, 
            trunclower = sh), 
     sort(secura$Loss), 
     main = "ME QQ-plot", 
     xlab = "Fitted quantiles", 
     ylab = "Empirical quantiles")
abline(0, 1)
```

]

---

class: clear

.pull-left[

Next, we combine what we learned on splicing with the Mixture of Erlangs distribution, to construct .KULbginline[a ME-Pa splicing model] with the splicing point `threshold` estimated earlier on. 

Again, we start from $M=10$ Erlangs in the mixture part for the body. 

```{r eval=FALSE} 
# Fit ME-Pa splicing model
splicefit_sec <- ReIns::SpliceFitPareto(
                    secura$Loss, 
                    tsplice = threshold, 
                    M = 10, 
                    trunclower = sh)

# Summary
summary(splicefit_sec)
```

How would you describe the resulting model?

]

.pull-right[

.tiny[
```{r echo=FALSE} 
# Fit ME-Pa splicing model
splicefit_sec <- ReIns::SpliceFitPareto(secura$Loss, 
                                        tsplice = threshold, 
                                        M = 10, 
                                        trunclower = sh)

# Summary
summary(splicefit_sec)
```
]
]

---

class: clear

.pull-left[

The first way to assess the GoF is a plot of the ECDF and the fitted CDF.
```{r eval=FALSE} 
# ECDF plot
x <- seq(10^6, 10^7, 10^3)
SpliceECDF(x, secura$Loss, splicefit_sec, lwd = 2)
abline(v = splicefit_sec$t, lty = 2, lwd = 2)

# Zoomed ECDF plot
SpliceECDF(x, secura$Loss, splicefit_sec, 
           lwd = 2, ylim = c(0,0.3))
abline(v = splicefit_sec$t, lty = 2, lwd = 2)
```

]

.pull-right[

```{r echo=FALSE} 
# ECDF plot
x <- seq(10^6, 10^7, 10^3)
SpliceECDF(x, secura$Loss, splicefit_sec, lwd = 2)
abline(v = splicefit_sec$t, lty = 2, lwd = 2)

# Zoomed ECDF plot
# SpliceECDF(x, secura$Loss, splicefit_sec, lwd = 2, ylim = c(0,0.3))
# abline(v = splicefit_sec$t, lty = 2, lwd = 2)
```

]

---


class: clear

.pull-left[

The first way to assess the GoF is a plot of the ECDF and the fitted CDF.
```{r eval=FALSE} 
# ECDF plot
x <- seq(10^6, 10^7, 10^3)
SpliceECDF(x, secura$Loss, splicefit_sec, lwd = 2)
abline(v = splicefit_sec$t, lty = 2, lwd = 2)

# Zoomed ECDF plot
SpliceECDF(x, secura$Loss, splicefit_sec, 
           lwd = 2, ylim = c(0,0.3))
abline(v = splicefit_sec$t, lty = 2, lwd = 2)
```

]

.pull-right[

```{r echo=FALSE} 
# Zoomed ECDF plot
SpliceECDF(x, secura$Loss, splicefit_sec, lwd = 2, ylim = c(0,0.3))
abline(v = splicefit_sec$t, lty = 2, lwd = 2)
```

]


---

class: clear

.pull-left[

Additionally, a PP-plot can be used which plots the empirical survival function vs. the fitted survival function.
To focus on the tails, a version with minus log-scale is also used, i.e.

$$-\log(1-\hat{F}(x_{i,n}))\ \text{vs.}\ -\log(1-F(x_{i,n})).$$

```{r eval=FALSE} 
# PP-plot
SplicePP(secura$Loss, splicefit_sec)
# PP-plot with minus log-scale
SplicePP(secura$Loss, splicefit_sec, log = TRUE)
```



]

.pull-right[

```{r echo=FALSE, out.width='50%'} 
# PP-plot
SplicePP(secura$Loss, splicefit_sec)
```

```{r echo=FALSE, out.width='50%'} 
# PP-plot with minus log-scale
SplicePP(secura$Loss, splicefit_sec, log = TRUE)
```

]

---

class: clear

.pull-left[
Finally, a QQ-plot can be used again.
```{r eval=FALSE} 
# QQ-plot
SpliceQQ(secura$Loss, splicefit_sec)
```
]
.pull-right[
```{r echo=FALSE} 
# QQ-plot
SpliceQQ(secura$Loss, splicefit_sec)
```

]

---
# Risk measures

Some popular risk measures are

* Value-at-Risk

$$ VaR_{1-p} = F^{-1}(1-p)$$

* Value-at-Risk quantifies tail events.

<br>

* Conditional Tail Expectation 

\begin{align}
CTE_{1-p} &= E(X \mid X > VaR_{1-p}) \\
          &= \frac{1}{p} \int_{1-p}^{1} VaR_\gamma(X) d\gamma.
\end{align}

* Conditional Tail Expectation quantifies the expected severity of tail events (.KULbginline[what if?]).

---
# Risk measures for a body-tail fit

We give a general expression for the CDF and quantile function for the body-tail model:

$$\begin{eqnarray*}
f(x) &=& \begin{cases}
	0 & x \leq t^l \\
	\pi \cdot \color{blue}{\frac{f_1^{\star}(x)}{F_1^{\star}(t)-F_1^{\star}(t^l)}} & t^l \leq x \leq t \\
(1-\pi) \cdot \color{orange}{\frac{f_2^{\star}(x)}{1-F_2^{\star}(t)}} & x > t \end{cases}
\end{eqnarray*}$$

<br>

.pull-left[
The CDF is found by integrating the density:

$$
\begin{eqnarray*}
F(x) &=& \begin{cases}
	0 & x \leq t^l \\
	\pi \cdot \color{blue}{\frac{F_1^{\star}(x) - F_1^{\star}(t^l)}{F_1^{\star}(t)-F_1^{\star}(t^l)}} & t^l \leq x \leq t \\
\pi + (1-\pi) \cdot \color{orange}{\frac{F_2^{\star}(x) - F_2^{\star}(t)}{1-F_2^{\star}(t)}} & x > t \end{cases}
\end{eqnarray*}
$$
]

.pull-left[
The quantile function is obtained by inverting the CDF:
$$
\begin{eqnarray*}
F^{-1}(p) &=& \begin{cases}
Q_1^{*} \left( \frac{p}{\pi} \cdot \color{blue}{ (F_1^{\star}(t) - F_1^{\star}(t^l)) + F_1^{\star}(t^l)} \right)  & 0 \leq p \leq \pi \\
Q_2^{*} \left( \frac{p-\pi}{1-\pi} \cdot \color{orange}{(1 - F_2^{\star}(t)) +F_2^{\star}(t)} \right) & \pi \leq p \leq 1 \end{cases}
\end{eqnarray*}
$$
where $Q_1^{*}$ and $Q_2^{*}$ denote the quantile functions of the body resp. tail distribution used in the splicing model.
]
---
# Risk measures for a body-tail fit

```{r} 
# Quantile function for Exp-Pa splicing model
ExpPa_quantile <- function(p, sh, threshold, lambda, gamma) {
  
  pi <- (n-kopt)/n
  
  x <- numeric(length(p))
  
  x[p <= pi] <- qexp(p[p <= pi] / pi * (pexp(threshold, rate = lambda) - pexp(sh, rate = lambda)) + 
                       pexp(sh, rate = lambda), rate = lambda)
  
  x[p > pi] <- threshold * (1 - (p[p > pi] - pi) / (1-pi))^(-gamma)

  return(x)
}
```

---
# Risk measures for a body-tail fit

.pull-left[
95% Value-at-Risk

```{r}
ExpPa_quantile(0.95, sh, threshold, lambda, gamma)
```

and check

```{r}
ExpPa_cdf(ExpPa_quantile(0.95, sh, 
                         threshold, 
                         lambda, gamma), 
          sh, threshold, lambda, gamma)
```


]

.pull-right[
95% Conditional Tail Expectation

```{r}
qf <- function(p) {
  ExpPa_quantile(p, sh, threshold, lambda, gamma) 
}

int <- integrate(qf, lower = 0.95, upper = 1) 
int$value / 0.05
```

<br/>

The {ReIns} package has built-in functions to calculate these (and other) risk measures for the ME-Pa spliced fit, see e.g. `ReIns::VaR(p, splicefit_sec)` and `ReIns::CTE(p, splicefit_sec)`. 

]


---
name: wrap-up

# Thanks!  <img src="image/xaringan.png" class="title-hex">

<br>
<br>
<br>
<br>

Slides created with the R package [xaringan](https://github.com/yihui/xaringan).
<br> <br> <br>
Course material available via 
<br>
`r fa(name = "github", fill = KULbg)` https://github.com/katrienantonio/loss-models-2020


```{r, eval=FALSE, include=FALSE}
# this code can be used to extract the R code from an R Markdown (Rmd) document
library(knitr)
path <- "C:/Users/u0043788/Dropbox/Cursus Arcturus/Dag1/Material"
setwd(path)
file.exists("loss_modelling_reserving_analytics_day_1.Rmd")
purl("loss_modelling_reserving_analytics_day_1.Rmd")
```
